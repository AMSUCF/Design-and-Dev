{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First, import the libraries you need for reading in web content. Use the model in \"WebScraping.ipynb.\" Set the URL to a website of interest for your work: this can be anything, but will work best if it's a static site (not a social media hashtag, etc.)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Second, save the website you are interested in as a file. Name that file appropriately to the content - for instance, something like \"reviewarchive.html\" would make sense if you are pulling reviews of a movie or book. After you run it, check that the file looks like it has the content you need. If it doesn't, try to figure out what went wrong.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Third, append another page of connected content: you'll need to use a modified version of the previous code, changing the URL, and using \"append binary\" to add to the file.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bonus Challenge: If you scraped something repetitive (for instance, I used two review pages from Rotten Tomatoes as I was testing my solution), you'll notice you have a lot of redundancy in your file, and probably only a few things you really want. Can you structure your data to only save the part of the webContent pull you are interested in? (If you're new to programming or Python, or stressed for time this week, don't worry about the bonus rounds!)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Next, pull in the contents of the new file (with multiple pages of HTML) using our text file reading example from week one as a model. Store those contents in a new variable, named appropriately to the contents. Print a subset of the output to confirm the import was successful.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finally, use the examples in \"Conditionals.ipynb\" and \"PlayingWithStrings.ipynb\" to run at least five simple data analyses commands on your content and print the results with an explanatory statement, as in the examples. Here's a few suggestions to try, but feel free to expand on them.\r\n",
    "\r\n",
    "# 1. Count and print the occurences of a key word\r\n",
    "\r\n",
    "# 2. Split your dataset into strings based on word spacing, and print some of the words to view the set\r\n",
    "\r\n",
    "# 3. Locate the index of a word of interest, then print the phrase surrounding it\r\n",
    "\r\n",
    "# 4. Compare the number of occurences of two significant words, such as character names, using a conditional statement. The printed statement should be different for each outcome, and handle the case where the numbers are the same (try using if, elif, and else)\r\n",
    "\r\n",
    "# 5. Search for a word and use \"in\" to print true if the word is found\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}