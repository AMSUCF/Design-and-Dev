{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "corpus = [\"I am an invisible man\",\r\n",
    "          \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move\",\r\n",
    "          \"Mother died today. Or maybe, yesterday; I can't be sure\",\r\n",
    "          \"It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\",\r\n",
    "          \"Ships at a distance have every man’s wish on board\",\r\n",
    "          \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold\",\r\n",
    "          \"It was a bright cold day in April, and the clocks were striking thirteen\",\r\n",
    "          \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\"]\r\n",
    "\r\n",
    "document = corpus[2]\r\n",
    "print(document.split())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Mother', 'died', 'today.', 'Or', 'maybe,', 'yesterday;', 'I', \"can't\", 'be', 'sure']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# %%\r\n",
    "import nltk\r\n",
    "import nltk.tokenize\r\n",
    "\r\n",
    "# download the most recent punkt package\r\n",
    "nltk.download('punkt', quiet=True)\r\n",
    "\r\n",
    "document = corpus[3]\r\n",
    "print(nltk.tokenize.word_tokenize(document, language='english'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['It', 'was', 'a', 'queer', ',', 'sultry', 'summer', ',', 'the', 'summer', 'they', 'electrocuted', 'the', 'Rosenbergs', ',', 'and', 'I', 'didn', '’', 't', 'know', 'what', 'I', 'was', 'doing', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import re\r\n",
    "\r\n",
    "\r\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\r\n",
    "\r\n",
    "\r\n",
    "def is_punct(string):\r\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\r\n",
    "       punctuation markers.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        string (str): a string to check for punctuation markers.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        bool: True is string is a (sequence of) punctuation marker(s),\r\n",
    "            False otherwise.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> is_punct(\"!\")\r\n",
    "        True\r\n",
    "        >>> is_punct(\"Bonjour!\")\r\n",
    "        False\r\n",
    "        >>> is_punct(\"¿Te gusta el verano?\")\r\n",
    "        False\r\n",
    "        >>> is_punct(\"...\")\r\n",
    "        True\r\n",
    "        >>> is_punct(\"«»...\")\r\n",
    "        True\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    return PUNCT_RE.match(string) is not None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tokens = nltk.tokenize.word_tokenize(corpus[2], language='english')\r\n",
    "\r\n",
    "# Loop with a standard for-loop\r\n",
    "tokenized = []\r\n",
    "for token in tokens:\r\n",
    "    if not is_punct(token):\r\n",
    "        tokenized.append(token)\r\n",
    "print(tokenized)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Mother', 'died', 'today', 'Or', 'maybe', 'yesterday', 'I', 'ca', \"n't\", 'be', 'sure']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def preprocess_text(text, language, lowercase=True):\r\n",
    "    \"\"\"Preprocess a text.\r\n",
    "\r\n",
    "    Perform a text preprocessing procedure, which transforms a string\r\n",
    "    object into a list of word tokens without punctuation markers.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        text (str): a string representing a text.\r\n",
    "        language (str): a string specifying the language of text.\r\n",
    "        lowercase (bool, optional): Set to True to lowercase all\r\n",
    "            word tokens. Defaults to True.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: a list of word tokens extracted from text, excluding\r\n",
    "            punctuation.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> preprocess_text(\"Ah! Monsieur, c'est donc vous?\", 'french')\r\n",
    "        [\"ah\", \"monsieur\", \"c'est\", \"donc\", \"vous\"]\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    if lowercase:\r\n",
    "        text = text.lower()\r\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\r\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for document in corpus[2:4]:\r\n",
    "    print('Original:', document)\r\n",
    "    print('Tokenized:', preprocess_text(document, 'english'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original: Mother died today. Or maybe, yesterday; I can't be sure\n",
      "Tokenized: ['mother', 'died', 'today', 'or', 'maybe', 'yesterday', 'i', 'ca', \"n't\", 'be', 'sure']\n",
      "Original: It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\n",
      "Tokenized: ['it', 'was', 'a', 'queer', 'sultry', 'summer', 'the', 'summer', 'they', 'electrocuted', 'the', 'rosenbergs', 'and', 'i', 'didn', 't', 'know', 'what', 'i', 'was', 'doing', 'in', 'new', 'york']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import collections\r\n",
    "\r\n",
    "vocabulary = collections.Counter()\r\n",
    "for document in corpus:\r\n",
    "    vocabulary.update(preprocess_text(document, 'english'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(vocabulary.most_common(n=5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 9), ('a', 6), ('i', 4), ('in', 4), ('was', 4)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print('Original vocabulary size:', len(vocabulary))\r\n",
    "pruned_vocabulary = {token for token, count in vocabulary.items() if count > 1}\r\n",
    "print(pruned_vocabulary)\r\n",
    "print('Pruned vocabulary size:', len(pruned_vocabulary))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original vocabulary size: 100\n",
      "{'the', 'was', 'of', 'were', 'it', 'on', 'i', 'and', 'man', 'a', 'summer', 'as', 'in'}\n",
      "Pruned vocabulary size: 13\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\r\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\r\n",
    "            of lists of strings.\r\n",
    "        min_count (int, optional): the minimum occurrence count of a\r\n",
    "            vocabulary item in the corpus.\r\n",
    "        max_count (int, optional): the maximum occurrence count of a\r\n",
    "            vocabulary item in the corpus. Defaults to inf.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: An alphabetically ordered list of unique words in the\r\n",
    "            corpus, of which the frequencies adhere to the specified\r\n",
    "            minimum and maximum count.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\r\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\r\n",
    "                      ['a', 'story', 'book', 'drama']]\r\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\r\n",
    "        ['book', 'drama', 'love', 'man', 'the']\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    vocabulary = collections.Counter()\r\n",
    "    for document in tokenized_corpus:\r\n",
    "        vocabulary.update(document)\r\n",
    "    vocabulary = {word for word, count in vocabulary.items()\r\n",
    "                  if count >= min_count and count <= max_count}\r\n",
    "    return sorted(vocabulary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "tokenized_corpus = [preprocess_text(document, 'english') for document in corpus]\r\n",
    "vocabulary = extract_vocabulary(tokenized_corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "bags_of_words = []\r\n",
    "for document in tokenized_corpus:\r\n",
    "    tokens = [word for word in document if word in vocabulary]\r\n",
    "    bags_of_words.append(collections.Counter(tokens))\r\n",
    "\r\n",
    "print(bags_of_words[5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'the': 3, 'we': 1, 'were': 1, 'somewhere': 1, 'around': 1, 'barstow': 1, 'on': 1, 'edge': 1, 'of': 1, 'desert': 1, 'when': 1, 'drugs': 1, 'began': 1, 'to': 1, 'take': 1, 'hold': 1})\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('py38': conda)"
  },
  "interpreter": {
   "hash": "1e5a1de8262f0e9d1487335168cc67bdb3373dee2cd3d5c339b458468358fbed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}