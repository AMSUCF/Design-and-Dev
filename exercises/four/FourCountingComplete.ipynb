{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# For this exercise, you should start by reading in the text file produced from Exercise Three. This text file should already have the HTML code elements removed, and primarily consist of text and other characters that we will remove through our processing. Store the input of your file in a string, and convert the contents to lower case for consistency. \r\n",
    "\r\n",
    "f = open('dogememe.txt','r')\r\n",
    "fileText = str(f.read().lower())\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# This week, we'll be using functions that we've already defined in our exercises. Try looking back through your code and notes from class. Our first step is to import the regular expressions module and remove all non-alpha-numeric characters from the string, then save the string as an array of words ready for processing. (Use the code in NormalizingText for reference)\r\n",
    "\r\n",
    "def preprocess_text(text):\r\n",
    "    import re\r\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text)\r\n",
    "\r\n",
    "word_bag = preprocess_text(fileText)\r\n",
    "print(word_bag[0:50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['aboutdoge', 'pronounced', 'xcb', 'x88do', 'xca', 'x8ad', 'xca', 'x92', 'dohj', 'is', 'a', 'slang', 'term', 'for', 'dog', 'that', 'is', 'primarily', 'associated', 'with', 'pictures', 'of', 'shiba', 'inus', 'nicknamed', 'shibe', 'and', 'internal', 'monologue', 'captions', 'on', 'tumblr', 'these', 'photos', 'may', 'be', 'photoshopped', 'to', 'change', 'the', 'dog', 's', 'face', 'or', 'captioned', 'with', 'interior', 'monologues', 'in', 'comic']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Next, we'll use stop words to remove all the words that we don't want to include in our count. There are suggested words in this week's readings, but you can also generate a custom list based on your topic. Define the stop words in an array, and use a loop to remove any word in your bag of words that also appears on the stop words list (use the examples in Dictionaries for guidance.)\r\n",
    "\r\n",
    "stop_words = ['is','a','on','the','in','aboutdoge','xca','xcb','x92','of','or','be','was','to','s','n','x88do','x8ad']\r\n",
    "\r\n",
    "def removeStopWords(word_bag, stop_words):\r\n",
    "    return [w for w in word_bag if w not in stop_words]\r\n",
    "\r\n",
    "cleaned_words = removeStopWords(word_bag, stop_words)\r\n",
    "\r\n",
    "print(cleaned_words[0:50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pronounced', 'dohj', 'slang', 'term', 'for', 'dog', 'that', 'primarily', 'associated', 'with', 'pictures', 'shiba', 'inus', 'nicknamed', 'shibe', 'and', 'internal', 'monologue', 'captions', 'tumblr', 'these', 'photos', 'may', 'photoshopped', 'change', 'dog', 'face', 'captioned', 'with', 'interior', 'monologues', 'comic', 'sans', 'font', 'starting', '2017', 'ironic', 'doge', 'formats', 'gained', 'prevalence', 'over', 'original', 'wholesome', 'version', 'noriginthe', 'use', 'misspelled', 'word', 'doge']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Now we're ready to count our words, and move from an array to a dictionary. Using the functions we've already built in the \"Dictionaries.ipynb\" file, process your text by building a dictionary that zips words with their frequency, then removes redundancy by storing the data in the \"dictionary\" format.\r\n",
    "\r\n",
    "def wordsToDictionary(word_bag):\r\n",
    "    word_freq = [word_bag.count(word) for word in word_bag]\r\n",
    "    return dict(list(zip(word_bag,word_freq)))\r\n",
    "\r\n",
    "dict_words = wordsToDictionary(cleaned_words)\r\n",
    "\r\n",
    "print(dict_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'pronounced': 1, 'dohj': 1, 'slang': 1, 'term': 3, 'for': 9, 'dog': 10, 'that': 8, 'primarily': 1, 'associated': 1, 'with': 14, 'pictures': 1, 'shiba': 9, 'inus': 2, 'nicknamed': 1, 'shibe': 2, 'and': 30, 'internal': 2, 'monologue': 3, 'captions': 2, 'tumblr': 8, 'these': 2, 'photos': 7, 'may': 3, 'photoshopped': 2, 'change': 1, 'face': 1, 'captioned': 5, 'interior': 2, 'monologues': 1, 'comic': 2, 'sans': 2, 'font': 1, 'starting': 1, '2017': 3, 'ironic': 1, 'doge': 34, 'formats': 1, 'gained': 2, 'prevalence': 2, 'over': 4, 'original': 4, 'wholesome': 2, 'version': 2, 'noriginthe': 1, 'use': 6, 'misspelled': 1, 'word': 3, 'refer': 2, 'dates': 1, 'back': 1, 'june': 5, '24th': 2, '2005': 1, 'when': 2, 'it': 7, 'mentioned': 1, 'an': 10, 'episode': 2, 'homestar': 2, 'runner': 1, 'puppet': 1, 'show': 1, 'titled': 2, 'biz': 1, 'cas': 1, 'fri': 1, '1': 6, '2': 3, 'calls': 1, 'strong': 1, 'bad': 1, 'his': 3, 'd': 1, 'o': 1, 'g': 1, 'e': 1, 'while': 2, 'trying': 1, 'distract': 1, 'him': 1, 'from': 5, 'work': 1, 'identityon': 1, 'february': 1, '13th': 1, '2010': 2, 'japanese': 1, 'kindergarten': 1, 'teacher': 1, 'atsuko': 1, 'sato': 2, 'posted': 4, 'several': 2, 'her': 2, 'rescue': 1, 'adopted': 1, 'inu': 6, 'kabosu': 10, 'personal': 1, 'blog': 4, '38': 1, 'among': 2, 'included': 2, 'peculiar': 1, 'shot': 1, 'sitting': 1, 'couch': 1, 'glaring': 1, 'sideways': 1, 'at': 8, 'camera': 1, 'raised': 1, 'eyebrows': 1, 'shown': 6, 'below': 6, 'right': 1, 'nin': 2, 'december': 8, '2013': 8, 'shortly': 1, 'after': 3, 'breakout': 1, 'tech': 2, 'news': 2, 'site': 1, 'verge': 2, '39': 1, 'published': 2, 'article': 4, 'identifying': 1, 'as': 15, 'depicted': 1, 'meme': 5, 'addition': 2, 'also': 4, 'identified': 1, 'suki': 1, 'who': 4, 'lives': 1, 'san': 1, 'francisco': 1, 'based': 4, 'photographer': 1, 'jonathan': 1, 'fleming': 1, 'scarfed': 1, 'portrayed': 1, 'another': 2, 'popular': 1, 'instance': 1, 'nspreadon': 1, 'october': 1, '28th': 1, 'photo': 4, 'submitted': 4, 'r': 8, 'ads': 1, 'subreddit': 5, '3': 1, 'title': 3, 'lmbo': 1, 'look': 1, 'this': 1, 'fukkin': 1, 'where': 1, 'received': 3, '266': 1, 'upvotes': 1, '218': 1, 'points': 1, 'overall': 1, '48': 1, 'comments': 1, 'prior': 1, 'being': 3, 'archived': 1, 'sometime': 1, 'april': 3, '2012': 3, 'user': 2, 'leonsumbitches': 3, '15': 2, 'uploaded': 2, 'audio': 1, 'file': 1, 'computer': 1, 'reading': 2, 'passage': 2, 'written': 1, 'like': 3, 'commands': 1, 'turn': 1, 'adventure': 1, 'game': 1, 'about': 4, 'encountering': 1, 'paired': 1, 'woman': 1, 'patting': 1, 'head': 2, '16': 2, 'has': 1, 'more': 6, 'than': 6, '33': 1, '000': 2, 'notes': 1, 'july': 3, 'http': 1, 'dailydoge': 2, 'com': 2, 'post': 4, '21839788086': 1, 'you': 1, 'have': 3, 'encountered': 1, 'xef': 5, 'xbd': 5, 'x81': 1, 'x84': 1, 'x8f': 1, 'x87': 1, 'x85': 1, 'response': 1, 'single': 4, 'topic': 4, 'your': 2, 'daily': 3, '17': 1, 'created': 3, 'but': 2, 'quickly': 1, 'abandoned': 1, 'reblogging': 1, 'times': 1, '7th': 1, 'youtuber': 1, 'kwandaoren66': 1, 'video': 3, 'person': 1, 'text': 2, 'fake': 1, 'pokemon': 1, 'battle': 1, '4': 2, 'by': 6, 'threads': 1, 'which': 4, 'numerous': 1, 'people': 2, 'shared': 1, 'dogs': 2, 'different': 1, 'outfits': 1, 'began': 3, 'appearing': 1, '4chan': 5, 'boards': 1, 'including': 3, 'v': 1, '11': 1, 'games': 1, 'same': 3, 'month': 2, 'photoset': 1, 'cup': 1, 'saucer': 1, 'balanced': 1, 'went': 1, 'viral': 1, 'airpi': 1, '14': 1, 'referred': 1, 'polite': 1, 'august': 3, 'first': 3, 'fuck': 1, 'yeah': 1, '13': 2, 'launched': 3, 'growth': 1, 'summer': 1, 'coincided': 1, 'popularity': 3, 'confessions': 1, 'shibes': 1, '18': 1, 'appeared': 1, 'reddit': 4, 'dogsiwannahug': 1, '6': 1, 'schnauzer': 1, 'cheezburger': 1, '5': 1, 'schnauze': 1, 'january': 1, '8th': 2, '7': 2, 'sharing': 2, 'videos': 1, '9': 1, 'share': 1, 'one': 3, 'day': 4, 'not': 2, '12': 2, 'dedicated': 2, '29th': 1, 'thread': 2, '19': 1, 'sticky': 1, 'featured': 1, 'top': 1, 'board': 1, 's4s': 1, 'shit': 1, 'says': 1, 'garnering': 1, '600': 4, 'replies': 1, 'november': 1, '20': 2, 'youtube': 1, 'implemented': 1, 'easter': 1, 'egg': 1, 'changes': 1, 'colored': 1, 'much': 1, 'style': 2, 'searches': 1, 'phrase': 1, '22': 2, 'tag': 1, '10': 1, 'often': 2, 'contains': 1, 'images': 3, 'whose': 1, 'facial': 1, 'features': 1, 'been': 3, 'manipulated': 1, 'similar': 2, 'starecat': 1, 'n4chan': 1, 'murica': 3, 'raidon': 1, 'evening': 1, '26th': 1, 'group': 1, 'users': 2, 'raided': 1, 'against': 1, 'tongue': 1, 'cheek': 1, 'patriotic': 1, 'forum': 2, 'flooding': 1, 'frontpage': 1, 'dozens': 1, 'depicting': 1, 'character': 2, 'all': 3, 'american': 1, 'themed': 1, 'settings': 1, 'according': 2, 'dot': 2, 'report': 1, '21': 2, 'raid': 1, 'started': 1, 'late': 1, 'sunday': 1, 'night': 1, 'lasted': 1, 'until': 1, 'p': 1, 'm': 1, 'et': 1, '27th': 1, 'point': 1, 'moderators': 1, 'removing': 1, 'related': 1, 'posts': 1, 'index': 1, 'unlike': 1, 'previous': 1, 'clashes': 1, 'between': 2, 'sudden': 1, 'influx': 1, 'reportedly': 1, 'met': 1, 'little': 1, 'opposition': 1, 'userbase': 1, 'reflecting': 1, 'growing': 1, 'outside': 1, 'ntrademark': 1, 'controversyon': 1, '2014': 2, 'california': 1, 'gaming': 1, 'accessory': 1, 'company': 2, 'filed': 2, 'trademark': 4, 'united': 1, 'states': 1, 'patent': 1, 'office': 1, 'sell': 1, 'card': 2, 'boxes': 1, 'playing': 1, 'covers': 1, 'likeness': 1, 'front': 1, '23rd': 2, '41': 1, 'filing': 2, 'statement': 2, 'ultra': 1, 'pro': 1, 'general': 1, 'manager': 1, 'jay': 1, 'kuo': 1, 'claimed': 1, 'protect': 1, 'sued': 1, 'they': 1, 'would': 1, 'allow': 1, 'royalty': 1, 'free': 1, 'any': 2, 'vendor': 1, 'wishes': 1, 'mark': 1, 'contained': 2, 'intellectual': 1, 'property': 2, 'director': 1, 'electronic': 1, 'frontier': 1, 'foundation': 1, 'corynne': 1, 'mcsherry': 1, 'noted': 1, 'problematic': 1, 'because': 1, 'attempts': 1, 'entire': 2, 'just': 2, 'stylized': 1, 'death': 1, 'hoaxon': 1, 'march': 1, '31st': 1, 'twitter': 5, 'account': 3, 'cctv': 1, 'reported': 1, 'had': 2, 'died': 1, 'age': 1, '42': 1, 'tweet': 3, '8': 1, '400': 2, 'retweets': 1, '100': 3, 'likes': 3, 'nhowever': 1, 'fools': 2, 'prank': 1, 'official': 4, 'instagram': 1, 'kabosumama': 2, 'alive': 2, 'well': 1, 'views': 1, '43': 1, 'caption': 1, 'ok': 1, 'she': 2, 'i': 1, 'don': 1, 't': 1, 'care': 1, 'somebody': 1, 'told': 1, 'lie': 1, 'dead': 1, 'important': 1, 'thing': 1, 'me': 1, 'girl': 1, 'female': 1, 'dogecoinon': 1, '6th': 1, 'bitcointalk': 1, 'member': 1, 'dogecoin': 9, 'introduced': 1, 'alternative': 1, 'cryptocurrency': 2, 'satire': 1, 'bitcoin': 4, 'boom': 1, '23': 1, 'xe2': 4, 'x80': 4, 'x93': 4, 'very': 1, 'currency': 2, 'many': 2, 'coin': 2, 'wow': 1, 'v1': 1, 'released': 1, 'its': 2, 'derivatives': 1, 'can': 1, 'mined': 2, 'exchanged': 1, 'goods': 1, 'services': 1, 'participants': 1, 'though': 1, 'programmed': 1, 'level': 1, 'out': 2, 'higher': 1, 'threshold': 1, 'up': 3, 'billion': 2, 'coins': 4, 'prevent': 1, 'special': 1, 'mining': 1, 'equipment': 1, 'asics': 1, 'comparison': 1, 'will': 2, 'cap': 1, 'million': 3, 'litecoin': 1, 'support': 1, '84': 1, 'circulation': 1, 'nfollowing': 1, 'launch': 1, 'website': 1, '24': 1, 'slew': 1, 'social': 1, 'media': 1, 'channels': 1, 'referential': 1, 'webpages': 1, 'soon': 1, 'emerged': 1, '25': 1, 'facebook': 2, 'page': 1, '26': 1, 'racking': 1, 'followers': 1, '800': 1, 'within': 1, 'week': 2, 'respectively': 1, 'community': 1, 'satirical': 2, '29': 1, 'accruing': 1, 'subscribers': 1, 'nthroughout': 1, 'weekend': 1, 'highlighted': 2, 'number': 1, 'sites': 1, 'blogs': 1, 'providing': 1, 'further': 1, 'boost': 1, 'value': 2, '14th': 1, 'encyclopedic': 1, '27': 1, 'describing': 1, 'concept': 1, 'wikipedia': 1, 'editor': 1, 'citationneeded': 1, 'peak': 1, 'estimated': 1, 'skyrocketed': 1, 'high': 1, '80': 1, 'per': 1, '15th': 1, 'worth': 1, '0': 2, '0002': 1, 'usd': 2, 'market': 1, 'capitalization': 1, '45': 1, 'total': 1, 'having': 1, '30': 1, 'npolitical': 1, 'examplesseveral': 1, 'republican': 2, 'politicians': 1, 'image': 5, 'macros': 3, 'their': 1, 'feeds': 1, 'congressional': 1, 'representatives': 1, 'steve': 1, 'stockman': 1, 'thomas': 1, 'massie': 1, 'political': 1, '9th': 1, 'national': 1, 'committee': 1, 'feed': 2, 'macro': 1, 'mocking': 2, 'bill': 1, 'hillary': 1, 'clinton': 1, 'democratic': 1, 'party': 1, 'replied': 1, 'dated': 1, 'policies': 1, 'nironic': 1, 'memesironic': 1, 'memes': 2, 'are': 1, 'feature': 2, 'strange': 1, 'surreal': 1, 'circumstances': 1, 'take': 1, 'form': 1, 'edits': 1, 'purposes': 1, 'dark': 1, 'absurd': 1, 'humor': 1, 'format': 2, 'popularized': 1, 'deep': 1, 'ifunny': 1, '2016': 1, 'reached': 1, 'mainstream': 1, '2018': 1, 'gaining': 1, 'nvarious': 1, 'examples': 1, 'nsearch': 1, 'interesteditors': 1, 'note': 1, 'search': 1, 'interest': 1, 'partially': 1, 'inconsistent': 1, 'venetian': 1, 'leader': 1, 'used': 1, 'some': 1, 'italian': 1, 'crowned': 1, 'republics': 1, 'genoa': 1, 'venice': 1, 'trends': 3, 'embed': 2, 'renderexplorewidget': 1, 'timeseries': 1, 'comparisonitem': 1, 'keyword': 1, 'geo': 1, 'time': 1, 'category': 1, 'explorequery': 1, 'date': 1, 'q': 1, 'guestpath': 1, 'https': 1, 'google': 1, '443': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Finally, explore what you can learn from this dictionary. Try: \r\n",
    "# A. Sorting your dictionary using our prebuilt method\r\n",
    "def sortDictionary(words):\r\n",
    "    aux = [(words[key], key) for key in words]\r\n",
    "    aux.sort()\r\n",
    "    aux.reverse()\r\n",
    "    return aux\r\n",
    "\r\n",
    "sort_dict = sortDictionary(dict_words)\r\n",
    "\r\n",
    "# B. Printing the top five most frequent words from your data\r\n",
    "x = 0\r\n",
    "for pair in sort_dict:\r\n",
    "    print(str(pair))\r\n",
    "    x += 1\r\n",
    "    if x==4:\r\n",
    "        break\r\n",
    "\r\n",
    "# C. Querying for certain key words and printing their frequency\r\n",
    "\r\n",
    "print(\"Meme frequency: \" + str(dict_words.get(\"meme\")))\r\n",
    "print(\"Tumblr frequency: \" + str(dict_words.get(\"tumblr\")))\r\n",
    "\r\n",
    "# D. Comparing the relative frequency of words of interest\r\n",
    "\r\n",
    "def compare_words(word_one, word_two):\r\n",
    "    if not word_one in dict_words or not word_two in dict_words:\r\n",
    "       print(\"Word(s) not found\")\r\n",
    "    elif dict_words[word_one] >  dict_words[word_two]:\r\n",
    "        print(word_one + \" appeared more often\")\r\n",
    "    elif dict_words[word_two] >  dict_words[word_one]:\r\n",
    "        print(word_two + \" appeared more often\") \r\n",
    "    else:\r\n",
    "        print(\"Words occurred with equal frequency\")\r\n",
    "\r\n",
    "compare_words(\"doge\",\"meme\")\r\n",
    "compare_words(\"doge\",\"fred\")\r\n",
    "compare_words(\"royalty\",\"free\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(34, 'doge')\n",
      "(30, 'and')\n",
      "(15, 'as')\n",
      "(14, 'with')\n",
      "Meme frequency: 5\n",
      "Tumblr frequency: 8\n",
      "doge appeared more often\n",
      "Word(s) not found\n",
      "Words occurred with equal frequency\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}